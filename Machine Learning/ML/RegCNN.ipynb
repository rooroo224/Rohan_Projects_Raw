{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,BatchNormalization,Flatten,Conv1D,MaxPooling1D,Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "from numpy import array\n",
    "from numpy import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1030\n",
      "2030\n",
      "3030\n"
     ]
    }
   ],
   "source": [
    "blocks = [1030, 2030, 3030]\n",
    "#blocks = [1030, 2030, 3030, 4030, 5030, 6030, 7030]\n",
    "\n",
    "dfss = []\n",
    "for block in blocks:\n",
    "    print(block)\n",
    "    dfss.append(pd.read_csv('combined_df_'+str(block)+'.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfss)\n",
    "df = pd.read_csv('combined_df_'+str(1030)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(633401, 70)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    X = df[['Leading angle','Side tilt angle','Tool Tip Point X',\n",
    "           'Tool Tip Point Y', 'Tool Tip Point Z', 'Tool Orientation X',\n",
    "           'Tool Orientation Y', 'Tool Orientation Z',]].copy(deep=True).to_numpy()\n",
    "    y = df[['MachineX', 'MachineY', 'MachineZ', 'MachineA', 'MachineC']].copy(deep=True).to_numpy()\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(633401, 8) (633401, 5)\n",
      "(601, 400, 8) (601, 5)\n",
      "(601, 400, 8) (601, 5)\n",
      "(480, 50, 8) (480, 5)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 46, 600)           24600     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 46, 600)           2400      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 23, 600)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 14, 600)           3600600   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8400)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 150)               1260150   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 755       \n",
      "=================================================================\n",
      "Total params: 4,888,505\n",
      "Trainable params: 4,887,305\n",
      "Non-trainable params: 1,200\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 151295.78125, saving model to best_model.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 151295.78125 to 59676.81250, saving model to best_model.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 59676.81250 to 21366.05859, saving model to best_model.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 21366.05859 to 11772.92383, saving model to best_model.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 11772.92383 to 3062.42749, saving model to best_model.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 3062.42749 to 2188.23682, saving model to best_model.h5\n",
      "\n",
      "Epoch 00007: val_loss improved from 2188.23682 to 1712.26221, saving model to best_model.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 1712.26221 to 820.26410, saving model to best_model.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 820.26410\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 820.26410\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 820.26410\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 820.26410\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 820.26410\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 820.26410\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 820.26410\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 820.26410\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 820.26410\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 820.26410\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 820.26410\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 820.26410\n",
      "\n",
      "Epoch 00021: val_loss improved from 820.26410 to 414.00272, saving model to best_model.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 414.00272 to 398.60474, saving model to best_model.h5\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 398.60474\n",
      "\n",
      "Epoch 00024: val_loss improved from 398.60474 to 315.38239, saving model to best_model.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 315.38239\n",
      "\n",
      "Epoch 00057: val_loss improved from 315.38239 to 257.20633, saving model to best_model.h5\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 257.20633\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 257.20633\n",
      "\n",
      "Epoch 00060: val_loss improved from 257.20633 to 76.88045, saving model to best_model.h5\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 76.88045\n",
      "\n",
      "Epoch 00062: val_loss improved from 76.88045 to 61.33564, saving model to best_model.h5\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 61.33564\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 61.33564\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 61.33564\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 61.33564\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 61.33564\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 61.33564\n",
      "\n",
      "Epoch 00069: val_loss improved from 61.33564 to 45.93658, saving model to best_model.h5\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 45.93658\n",
      "\n",
      "Epoch 00091: val_loss improved from 45.93658 to 27.02539, saving model to best_model.h5\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 27.02539\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 27.02539\n",
      "\n",
      "Epoch 00094: val_loss improved from 27.02539 to 8.64900, saving model to best_model.h5\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 8.64900\n",
      "\n",
      "Epoch 00156: val_loss improved from 8.64900 to 5.06789, saving model to best_model.h5\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 5.06789\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 5.06789\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in):\n",
    "    X, y = list(), list()\n",
    "   \n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        \n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, 0:8], sequences[end_ix-1, 8:14]  # columns to see which are I/P and O/P features\n",
    "        \n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "# define input sequence\n",
    "X,y = get_dataset()\n",
    "print(X.shape,y.shape)\n",
    "dataset = np.concatenate((X[0:1000,:],y[0:1000,:]),axis=1)  \n",
    "#dataset = np.concatenate((X,y),axis=1)  \n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps_in = 400\n",
    "# convert into input/output\n",
    "X, y = split_sequences(dataset, n_steps_in)\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "num = 50\n",
    "size = np.floor(n_steps_in/num)\n",
    "\n",
    "sub_sampled = np.arange(0,n_steps_in,size, dtype=int)\n",
    "\n",
    "X_sampled = np.empty((X.shape[0],sub_sampled.shape[0],X.shape[2]))\n",
    "for i in np.arange(X.shape[0]):\n",
    "    X_sampled[i] = X[i][sub_sampled][:]\n",
    "\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sampled, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape,y_train.shape)\n",
    "\n",
    "n_ip_features = X_train.shape[2]              \n",
    "n_op_features = y_train.shape[1]\n",
    "#print(n_op_features)\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=600, kernel_size=5, activation='relu', input_shape=(num, n_ip_features)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(filters=600, kernel_size=10, activation='relu'))\n",
    "#model.add(Dropout(0.35))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(n_op_features))\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss='mse')   \n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=100, mode='min')\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "# fit model\n",
    "history = model.fit(X_train, y_train, epochs=2000,validation_data=(X_test,y_test), verbose=0,callbacks=[early_stopping,model_checkpoint], batch_size = 10)\n",
    "          \n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# demonstrate prediction\n",
    "#x_input = array([[70, 75], [80, 85], [90, 95]])\n",
    "#x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "#yhat = model.predict(x_input, verbose=0)\n",
    "#print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
