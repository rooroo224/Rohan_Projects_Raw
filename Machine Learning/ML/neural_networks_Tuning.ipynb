{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,BatchNormalization,Flatten,Conv1D,MaxPooling1D,Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "from numpy import array\n",
    "from numpy import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1030\n",
      "2030\n",
      "3030\n"
     ]
    }
   ],
   "source": [
    "blocks = [1030, 2030, 3030]\n",
    "#blocks = [1030, 2030, 3030, 4030, 5030, 6030, 7030]\n",
    "\n",
    "dfss = []\n",
    "for block in blocks:\n",
    "    print(block)\n",
    "    dfss.append(pd.read_csv('combined_df_'+str(block)+'.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1685106, 71)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat(dfss)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    X = df[['Leading angle','Side tilt angle','Tool Tip Point X',\n",
    "           'Tool Tip Point Y', 'Tool Tip Point Z', 'Tool Orientation X',\n",
    "           'Tool Orientation Y', 'Tool Orientation Z',]].copy(deep=True).to_numpy()\n",
    "    y = df[['MachineX', 'MachineY', 'MachineZ', 'MachineA', 'MachineC']].copy(deep=True).to_numpy()\n",
    "    \n",
    "    return X,y\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = get_dataset()\n",
    "n_inputs = X.shape[1]\n",
    "n_outputs = y.shape[1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = X_train[0:1000,:], X_test[0:1000,:], y_train[0:1000,:], y_test[0:1000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    "    \n",
    "    for i in range(hp.Int(\"num_layers\", 1, 10)):\n",
    "        model.add(Dense(units=hp.Int(\"units_\" + str(i), min_value=20, max_value=1000, step=20,default=160), activation=hp.Choice('dense_activation',values=['relu', 'tanh', 'sigmoid'],default='relu')))\n",
    "    \n",
    "    model.add(Dense(n_outputs))\n",
    "    \n",
    "    model.compile(optimizer= tf.keras.optimizers.Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG', default=1e-3)), loss=tf.keras.losses.MeanSquaredError() )  \n",
    "    return model\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=150, mode='min')\n",
    "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 19 Complete [00h 01m 24s]\n",
      "val_loss: 2386.947265625\n",
      "\n",
      "Best val_loss So Far: 2198.466796875\n",
      "Total elapsed time: 00h 49m 41s\n",
      "\n",
      "Search: Running Trial #20\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "num_layers        |7                 |4                 \n",
      "units_0           |220               |80                \n",
      "dense_activation  |sigmoid           |relu              \n",
      "learning_rate     |0.0015534         |0.00099064        \n",
      "units_1           |80                |620               \n",
      "units_2           |740               |960               \n",
      "units_3           |900               |920               \n",
      "units_4           |740               |400               \n",
      "units_5           |140               |680               \n",
      "units_6           |40                |540               \n",
      "units_7           |560               |540               \n",
      "units_8           |100               |580               \n",
      "\n",
      "Epoch 1/2000\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 16915.4512 - val_loss: 16628.9434\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 16628.94336, saving model to best_model.h5\n",
      "Epoch 2/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 15685.4121 - val_loss: 15461.4775\n",
      "\n",
      "Epoch 00002: val_loss improved from 16628.94336 to 15461.47754, saving model to best_model.h5\n",
      "Epoch 3/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 14599.7607 - val_loss: 14422.8271\n",
      "\n",
      "Epoch 00003: val_loss improved from 15461.47754 to 14422.82715, saving model to best_model.h5\n",
      "Epoch 4/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 13607.2441 - val_loss: 13422.8057\n",
      "\n",
      "Epoch 00004: val_loss improved from 14422.82715 to 13422.80566, saving model to best_model.h5\n",
      "Epoch 5/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 12638.6152 - val_loss: 12472.7852\n",
      "\n",
      "Epoch 00005: val_loss improved from 13422.80566 to 12472.78516, saving model to best_model.h5\n",
      "Epoch 6/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 11706.1279 - val_loss: 11558.4307\n",
      "\n",
      "Epoch 00006: val_loss improved from 12472.78516 to 11558.43066, saving model to best_model.h5\n",
      "Epoch 7/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 10865.0635 - val_loss: 10751.2803\n",
      "\n",
      "Epoch 00007: val_loss improved from 11558.43066 to 10751.28027, saving model to best_model.h5\n",
      "Epoch 8/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 10111.5400 - val_loss: 10024.9746\n",
      "\n",
      "Epoch 00008: val_loss improved from 10751.28027 to 10024.97461, saving model to best_model.h5\n",
      "Epoch 9/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 9427.4229 - val_loss: 9360.7725\n",
      "\n",
      "Epoch 00009: val_loss improved from 10024.97461 to 9360.77246, saving model to best_model.h5\n",
      "Epoch 10/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 8803.2773 - val_loss: 8753.2266\n",
      "\n",
      "Epoch 00010: val_loss improved from 9360.77246 to 8753.22656, saving model to best_model.h5\n",
      "Epoch 11/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 8232.5127 - val_loss: 8198.6787\n",
      "\n",
      "Epoch 00011: val_loss improved from 8753.22656 to 8198.67871, saving model to best_model.h5\n",
      "Epoch 12/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 7710.4810 - val_loss: 7687.1729\n",
      "\n",
      "Epoch 00012: val_loss improved from 8198.67871 to 7687.17285, saving model to best_model.h5\n",
      "Epoch 13/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 7231.9805 - val_loss: 7219.3062\n",
      "\n",
      "Epoch 00013: val_loss improved from 7687.17285 to 7219.30615, saving model to best_model.h5\n",
      "Epoch 14/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 6793.9482 - val_loss: 6789.4902\n",
      "\n",
      "Epoch 00014: val_loss improved from 7219.30615 to 6789.49023, saving model to best_model.h5\n",
      "Epoch 15/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 6386.6392 - val_loss: 6367.9038\n",
      "\n",
      "Epoch 00015: val_loss improved from 6789.49023 to 6367.90381, saving model to best_model.h5\n",
      "Epoch 16/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 5971.5449 - val_loss: 5962.5830\n",
      "\n",
      "Epoch 00016: val_loss improved from 6367.90381 to 5962.58301, saving model to best_model.h5\n",
      "Epoch 17/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 5607.1050 - val_loss: 5610.5010\n",
      "\n",
      "Epoch 00017: val_loss improved from 5962.58301 to 5610.50098, saving model to best_model.h5\n",
      "Epoch 18/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 5282.9058 - val_loss: 5297.4141\n",
      "\n",
      "Epoch 00018: val_loss improved from 5610.50098 to 5297.41406, saving model to best_model.h5\n",
      "Epoch 19/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 4988.5327 - val_loss: 4990.9312\n",
      "\n",
      "Epoch 00019: val_loss improved from 5297.41406 to 4990.93115, saving model to best_model.h5\n",
      "Epoch 20/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 4689.0708 - val_loss: 4694.1909\n",
      "\n",
      "Epoch 00020: val_loss improved from 4990.93115 to 4694.19092, saving model to best_model.h5\n",
      "Epoch 21/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 4425.8848 - val_loss: 4439.9434\n",
      "\n",
      "Epoch 00021: val_loss improved from 4694.19092 to 4439.94336, saving model to best_model.h5\n",
      "Epoch 22/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 4194.8740 - val_loss: 4213.8052\n",
      "\n",
      "Epoch 00022: val_loss improved from 4439.94336 to 4213.80518, saving model to best_model.h5\n",
      "Epoch 23/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 3987.9602 - val_loss: 4010.4055\n",
      "\n",
      "Epoch 00023: val_loss improved from 4213.80518 to 4010.40552, saving model to best_model.h5\n",
      "Epoch 24/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 3801.3860 - val_loss: 3826.2634\n",
      "\n",
      "Epoch 00024: val_loss improved from 4010.40552 to 3826.26343, saving model to best_model.h5\n",
      "Epoch 25/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 3633.1724 - val_loss: 3660.2751\n",
      "\n",
      "Epoch 00025: val_loss improved from 3826.26343 to 3660.27515, saving model to best_model.h5\n",
      "Epoch 26/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 3481.6724 - val_loss: 3507.9766\n",
      "\n",
      "Epoch 00026: val_loss improved from 3660.27515 to 3507.97656, saving model to best_model.h5\n",
      "Epoch 27/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 3344.6716 - val_loss: 3372.7869\n",
      "\n",
      "Epoch 00027: val_loss improved from 3507.97656 to 3372.78687, saving model to best_model.h5\n",
      "Epoch 28/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 3221.8208 - val_loss: 3248.9146\n",
      "\n",
      "Epoch 00028: val_loss improved from 3372.78687 to 3248.91455, saving model to best_model.h5\n",
      "Epoch 29/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 3111.7925 - val_loss: 3138.7734\n",
      "\n",
      "Epoch 00029: val_loss improved from 3248.91455 to 3138.77344, saving model to best_model.h5\n",
      "Epoch 30/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 3013.4985 - val_loss: 3041.0032\n",
      "\n",
      "Epoch 00030: val_loss improved from 3138.77344 to 3041.00317, saving model to best_model.h5\n",
      "Epoch 31/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2926.6008 - val_loss: 2951.3848\n",
      "\n",
      "Epoch 00031: val_loss improved from 3041.00317 to 2951.38477, saving model to best_model.h5\n",
      "Epoch 32/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2849.6865 - val_loss: 2873.7681\n",
      "\n",
      "Epoch 00032: val_loss improved from 2951.38477 to 2873.76807, saving model to best_model.h5\n",
      "Epoch 33/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2781.7583 - val_loss: 2806.1282\n",
      "\n",
      "Epoch 00033: val_loss improved from 2873.76807 to 2806.12817, saving model to best_model.h5\n",
      "Epoch 34/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2722.5042 - val_loss: 2745.6958\n",
      "\n",
      "Epoch 00034: val_loss improved from 2806.12817 to 2745.69580, saving model to best_model.h5\n",
      "Epoch 35/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2670.8723 - val_loss: 2693.1724\n",
      "\n",
      "Epoch 00035: val_loss improved from 2745.69580 to 2693.17236, saving model to best_model.h5\n",
      "Epoch 36/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2626.2065 - val_loss: 2646.0686\n",
      "\n",
      "Epoch 00036: val_loss improved from 2693.17236 to 2646.06860, saving model to best_model.h5\n",
      "Epoch 37/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2587.6091 - val_loss: 2607.3193\n",
      "\n",
      "Epoch 00037: val_loss improved from 2646.06860 to 2607.31934, saving model to best_model.h5\n",
      "Epoch 38/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2554.7485 - val_loss: 2572.6316\n",
      "\n",
      "Epoch 00038: val_loss improved from 2607.31934 to 2572.63159, saving model to best_model.h5\n",
      "Epoch 39/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2526.6694 - val_loss: 2543.5742\n",
      "\n",
      "Epoch 00039: val_loss improved from 2572.63159 to 2543.57422, saving model to best_model.h5\n",
      "Epoch 40/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2503.4746 - val_loss: 2517.4724\n",
      "\n",
      "Epoch 00040: val_loss improved from 2543.57422 to 2517.47241, saving model to best_model.h5\n",
      "Epoch 41/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2483.8628 - val_loss: 2497.3706\n",
      "\n",
      "Epoch 00041: val_loss improved from 2517.47241 to 2497.37061, saving model to best_model.h5\n",
      "Epoch 42/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2467.6016 - val_loss: 2478.2002\n",
      "\n",
      "Epoch 00042: val_loss improved from 2497.37061 to 2478.20020, saving model to best_model.h5\n",
      "Epoch 43/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2454.1003 - val_loss: 2462.8401\n",
      "\n",
      "Epoch 00043: val_loss improved from 2478.20020 to 2462.84009, saving model to best_model.h5\n",
      "Epoch 44/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2443.3264 - val_loss: 2451.6621\n",
      "\n",
      "Epoch 00044: val_loss improved from 2462.84009 to 2451.66211, saving model to best_model.h5\n",
      "Epoch 45/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2434.6868 - val_loss: 2440.1616\n",
      "\n",
      "Epoch 00045: val_loss improved from 2451.66211 to 2440.16162, saving model to best_model.h5\n",
      "Epoch 46/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2427.6069 - val_loss: 2431.3938\n",
      "\n",
      "Epoch 00046: val_loss improved from 2440.16162 to 2431.39380, saving model to best_model.h5\n",
      "Epoch 47/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2421.9475 - val_loss: 2425.5034\n",
      "\n",
      "Epoch 00047: val_loss improved from 2431.39380 to 2425.50342, saving model to best_model.h5\n",
      "Epoch 48/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2417.5674 - val_loss: 2418.2498\n",
      "\n",
      "Epoch 00048: val_loss improved from 2425.50342 to 2418.24976, saving model to best_model.h5\n",
      "Epoch 49/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2413.7437 - val_loss: 2414.4136\n",
      "\n",
      "Epoch 00049: val_loss improved from 2418.24976 to 2414.41357, saving model to best_model.h5\n",
      "Epoch 50/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2411.0068 - val_loss: 2410.4822\n",
      "\n",
      "Epoch 00050: val_loss improved from 2414.41357 to 2410.48218, saving model to best_model.h5\n",
      "Epoch 51/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2408.8875 - val_loss: 2406.6414\n",
      "\n",
      "Epoch 00051: val_loss improved from 2410.48218 to 2406.64136, saving model to best_model.h5\n",
      "Epoch 52/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2407.3972 - val_loss: 2403.5173\n",
      "\n",
      "Epoch 00052: val_loss improved from 2406.64136 to 2403.51733, saving model to best_model.h5\n",
      "Epoch 53/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2405.9971 - val_loss: 2401.5549\n",
      "\n",
      "Epoch 00053: val_loss improved from 2403.51733 to 2401.55493, saving model to best_model.h5\n",
      "Epoch 54/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2405.3984 - val_loss: 2400.1777\n",
      "\n",
      "Epoch 00054: val_loss improved from 2401.55493 to 2400.17773, saving model to best_model.h5\n",
      "Epoch 55/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2404.3794 - val_loss: 2398.5449\n",
      "\n",
      "Epoch 00055: val_loss improved from 2400.17773 to 2398.54492, saving model to best_model.h5\n",
      "Epoch 56/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2403.9119 - val_loss: 2397.3813\n",
      "\n",
      "Epoch 00056: val_loss improved from 2398.54492 to 2397.38135, saving model to best_model.h5\n",
      "Epoch 57/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2403.5701 - val_loss: 2396.3757\n",
      "\n",
      "Epoch 00057: val_loss improved from 2397.38135 to 2396.37573, saving model to best_model.h5\n",
      "Epoch 58/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2403.2302 - val_loss: 2395.4338\n",
      "\n",
      "Epoch 00058: val_loss improved from 2396.37573 to 2395.43384, saving model to best_model.h5\n",
      "Epoch 59/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.9211 - val_loss: 2394.8901\n",
      "\n",
      "Epoch 00059: val_loss improved from 2395.43384 to 2394.89014, saving model to best_model.h5\n",
      "Epoch 60/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.7295 - val_loss: 2394.1895\n",
      "\n",
      "Epoch 00060: val_loss improved from 2394.89014 to 2394.18945, saving model to best_model.h5\n",
      "Epoch 61/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5276 - val_loss: 2393.8279\n",
      "\n",
      "Epoch 00061: val_loss improved from 2394.18945 to 2393.82788, saving model to best_model.h5\n",
      "Epoch 62/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.6016 - val_loss: 2393.1497\n",
      "\n",
      "Epoch 00062: val_loss improved from 2393.82788 to 2393.14966, saving model to best_model.h5\n",
      "Epoch 63/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4062 - val_loss: 2392.8110\n",
      "\n",
      "Epoch 00063: val_loss improved from 2393.14966 to 2392.81104, saving model to best_model.h5\n",
      "Epoch 64/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.9553 - val_loss: 2393.0015\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 2392.81104\n",
      "Epoch 65/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4297 - val_loss: 2392.2244\n",
      "\n",
      "Epoch 00065: val_loss improved from 2392.81104 to 2392.22437, saving model to best_model.h5\n",
      "Epoch 66/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4153 - val_loss: 2392.0837\n",
      "\n",
      "Epoch 00066: val_loss improved from 2392.22437 to 2392.08374, saving model to best_model.h5\n",
      "Epoch 67/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3765 - val_loss: 2391.9346\n",
      "\n",
      "Epoch 00067: val_loss improved from 2392.08374 to 2391.93457, saving model to best_model.h5\n",
      "Epoch 68/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3459 - val_loss: 2391.7295\n",
      "\n",
      "Epoch 00068: val_loss improved from 2391.93457 to 2391.72949, saving model to best_model.h5\n",
      "Epoch 69/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5520 - val_loss: 2392.1101\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 2391.72949\n",
      "Epoch 70/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4690 - val_loss: 2391.2749\n",
      "\n",
      "Epoch 00070: val_loss improved from 2391.72949 to 2391.27490, saving model to best_model.h5\n",
      "Epoch 71/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3401 - val_loss: 2391.6157\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 2391.27490\n",
      "Epoch 72/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2012 - val_loss: 2391.6135\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 2391.27490\n",
      "Epoch 73/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3716 - val_loss: 2391.5127\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 2391.27490\n",
      "Epoch 74/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3530 - val_loss: 2391.2468\n",
      "\n",
      "Epoch 00074: val_loss improved from 2391.27490 to 2391.24683, saving model to best_model.h5\n",
      "Epoch 75/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.1462 - val_loss: 2391.5269\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 2391.24683\n",
      "Epoch 76/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5020 - val_loss: 2391.4756\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 2391.24683\n",
      "Epoch 77/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4404 - val_loss: 2391.6421\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 2391.24683\n",
      "Epoch 78/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5159 - val_loss: 2391.2043\n",
      "\n",
      "Epoch 00078: val_loss improved from 2391.24683 to 2391.20435, saving model to best_model.h5\n",
      "Epoch 79/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4260 - val_loss: 2391.3789\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 2391.20435\n",
      "Epoch 80/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2834 - val_loss: 2391.0457\n",
      "\n",
      "Epoch 00080: val_loss improved from 2391.20435 to 2391.04565, saving model to best_model.h5\n",
      "Epoch 81/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3772 - val_loss: 2391.2500\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 2391.04565\n",
      "Epoch 82/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4338 - val_loss: 2391.1370\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 2391.04565\n",
      "Epoch 83/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3110 - val_loss: 2391.2129\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 2391.04565\n",
      "Epoch 84/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4978 - val_loss: 2391.2363\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 2391.04565\n",
      "Epoch 85/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5996 - val_loss: 2390.8782\n",
      "\n",
      "Epoch 00085: val_loss improved from 2391.04565 to 2390.87817, saving model to best_model.h5\n",
      "Epoch 86/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3704 - val_loss: 2391.0430\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 2390.87817\n",
      "Epoch 87/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3164 - val_loss: 2391.0435\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 2390.87817\n",
      "Epoch 88/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4043 - val_loss: 2390.9001\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 2390.87817\n",
      "Epoch 89/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2961 - val_loss: 2391.0974\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 2390.87817\n",
      "Epoch 90/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5442 - val_loss: 2391.1418\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 2390.87817\n",
      "Epoch 91/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.6611 - val_loss: 2391.1985\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 2390.87817\n",
      "Epoch 92/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.6328 - val_loss: 2391.3948\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 2390.87817\n",
      "Epoch 93/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4109 - val_loss: 2390.9307\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 2390.87817\n",
      "Epoch 94/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2664 - val_loss: 2391.2173\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 2390.87817\n",
      "Epoch 95/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5000 - val_loss: 2390.9758\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 2390.87817\n",
      "Epoch 96/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4060 - val_loss: 2391.2483\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 2390.87817\n",
      "Epoch 97/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.8279 - val_loss: 2390.7434\n",
      "\n",
      "Epoch 00097: val_loss improved from 2390.87817 to 2390.74341, saving model to best_model.h5\n",
      "Epoch 98/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4453 - val_loss: 2391.0696\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 2390.74341\n",
      "Epoch 99/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4241 - val_loss: 2391.3113\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 2390.74341\n",
      "Epoch 100/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3484 - val_loss: 2391.0596\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 2390.74341\n",
      "Epoch 101/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3723 - val_loss: 2390.9236\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 2390.74341\n",
      "Epoch 102/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5803 - val_loss: 2391.3389\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 2390.74341\n",
      "Epoch 103/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4136 - val_loss: 2391.0283\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 2390.74341\n",
      "Epoch 104/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3247 - val_loss: 2390.9722\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 2390.74341\n",
      "Epoch 105/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2805 - val_loss: 2391.1270\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 2390.74341\n",
      "Epoch 106/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5820 - val_loss: 2391.3582\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 2390.74341\n",
      "Epoch 107/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.7043 - val_loss: 2390.8005\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 2390.74341\n",
      "Epoch 108/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4404 - val_loss: 2390.8064\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 2390.74341\n",
      "Epoch 109/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3379 - val_loss: 2391.0876\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 2390.74341\n",
      "Epoch 110/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4060 - val_loss: 2391.0222\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 2390.74341\n",
      "Epoch 111/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4082 - val_loss: 2391.0537\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 2390.74341\n",
      "Epoch 112/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4036 - val_loss: 2391.0254\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 2390.74341\n",
      "Epoch 113/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3101 - val_loss: 2390.8054\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 2390.74341\n",
      "Epoch 114/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4546 - val_loss: 2390.9434\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 2390.74341\n",
      "Epoch 115/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2305 - val_loss: 2390.9573\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 2390.74341\n",
      "Epoch 116/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5212 - val_loss: 2390.9712\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 2390.74341\n",
      "Epoch 117/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3379 - val_loss: 2390.9490\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 2390.74341\n",
      "Epoch 118/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2854 - val_loss: 2391.1851\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 2390.74341\n",
      "Epoch 119/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2034 - val_loss: 2390.8533\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 2390.74341\n",
      "Epoch 120/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3403 - val_loss: 2391.1111\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 2390.74341\n",
      "Epoch 121/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3750 - val_loss: 2390.9836\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 2390.74341\n",
      "Epoch 122/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3474 - val_loss: 2391.0513\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 2390.74341\n",
      "Epoch 123/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.7307 - val_loss: 2390.6533\n",
      "\n",
      "Epoch 00123: val_loss improved from 2390.74341 to 2390.65332, saving model to best_model.h5\n",
      "Epoch 124/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5659 - val_loss: 2391.2051\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 2390.65332\n",
      "Epoch 125/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5300 - val_loss: 2390.9800\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 2390.65332\n",
      "Epoch 126/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3000 - val_loss: 2391.1343\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 2390.65332\n",
      "Epoch 127/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2403.0195 - val_loss: 2391.9722\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 2390.65332\n",
      "Epoch 128/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.1865 - val_loss: 2391.2795\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 2390.65332\n",
      "Epoch 129/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4836 - val_loss: 2391.0295\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 2390.65332\n",
      "Epoch 130/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5344 - val_loss: 2390.8274\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 2390.65332\n",
      "Epoch 131/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.1299 - val_loss: 2390.9397\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 2390.65332\n",
      "Epoch 132/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3511 - val_loss: 2391.1121\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 2390.65332\n",
      "Epoch 133/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4761 - val_loss: 2391.3350\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 2390.65332\n",
      "Epoch 134/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.1902 - val_loss: 2391.0701\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 2390.65332\n",
      "Epoch 135/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3782 - val_loss: 2390.7979\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 2390.65332\n",
      "Epoch 136/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3838 - val_loss: 2391.2217\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 2390.65332\n",
      "Epoch 137/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5081 - val_loss: 2391.1909\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 2390.65332\n",
      "Epoch 138/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4495 - val_loss: 2391.4185\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 2390.65332\n",
      "Epoch 139/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5254 - val_loss: 2391.4331\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 2390.65332\n",
      "Epoch 140/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4612 - val_loss: 2390.9558\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 2390.65332\n",
      "Epoch 141/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2478 - val_loss: 2391.2307\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 2390.65332\n",
      "Epoch 142/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4602 - val_loss: 2391.0632\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 2390.65332\n",
      "Epoch 143/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2734 - val_loss: 2391.0581\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 2390.65332\n",
      "Epoch 144/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5591 - val_loss: 2390.9336\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 2390.65332\n",
      "Epoch 145/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.6965 - val_loss: 2390.7310\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 2390.65332\n",
      "Epoch 146/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3525 - val_loss: 2390.9353\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 2390.65332\n",
      "Epoch 147/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2664 - val_loss: 2391.1873\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 2390.65332\n",
      "Epoch 148/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5273 - val_loss: 2390.8745\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 2390.65332\n",
      "Epoch 149/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.6445 - val_loss: 2391.0640\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 2390.65332\n",
      "Epoch 150/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2979 - val_loss: 2391.3130\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 2390.65332\n",
      "Epoch 151/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3308 - val_loss: 2390.6809\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 2390.65332\n",
      "Epoch 152/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.6289 - val_loss: 2391.3564\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 2390.65332\n",
      "Epoch 153/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3147 - val_loss: 2390.8276\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 2390.65332\n",
      "Epoch 154/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3765 - val_loss: 2391.2649\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 2390.65332\n",
      "Epoch 155/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3372 - val_loss: 2391.1975\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 2390.65332\n",
      "Epoch 156/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3096 - val_loss: 2391.0571\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 2390.65332\n",
      "Epoch 157/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2576 - val_loss: 2390.9873\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 2390.65332\n",
      "Epoch 158/2000\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2402.3857 - val_loss: 2391.0791\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 2390.65332\n",
      "Epoch 159/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.6995 - val_loss: 2391.0149\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 2390.65332\n",
      "Epoch 160/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3438 - val_loss: 2391.1335\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 2390.65332\n",
      "Epoch 161/2000\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 2402.3428 - val_loss: 2390.8857\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 2390.65332\n",
      "Epoch 162/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4148 - val_loss: 2390.9285\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 2390.65332\n",
      "Epoch 163/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3750 - val_loss: 2390.9102\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 2390.65332\n",
      "Epoch 164/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5674 - val_loss: 2391.6189\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 2390.65332\n",
      "Epoch 165/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3311 - val_loss: 2390.7808\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 2390.65332\n",
      "Epoch 166/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3584 - val_loss: 2390.5071\n",
      "\n",
      "Epoch 00166: val_loss improved from 2390.65332 to 2390.50708, saving model to best_model.h5\n",
      "Epoch 167/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3022 - val_loss: 2390.8770\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 2390.50708\n",
      "Epoch 168/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.7097 - val_loss: 2390.6213\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 2390.50708\n",
      "Epoch 169/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4604 - val_loss: 2391.2400\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 2390.50708\n",
      "Epoch 170/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.6895 - val_loss: 2391.4688\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 2390.50708\n",
      "Epoch 171/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3538 - val_loss: 2390.7681\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 2390.50708\n",
      "Epoch 172/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4075 - val_loss: 2391.3450\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 2390.50708\n",
      "Epoch 173/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5510 - val_loss: 2391.2341\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 2390.50708\n",
      "Epoch 174/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3286 - val_loss: 2390.9285\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 2390.50708\n",
      "Epoch 175/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4692 - val_loss: 2390.7839\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 2390.50708\n",
      "Epoch 176/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3794 - val_loss: 2391.0938\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 2390.50708\n",
      "Epoch 177/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2766 - val_loss: 2390.9944\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 2390.50708\n",
      "Epoch 178/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.6064 - val_loss: 2390.8042\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 2390.50708\n",
      "Epoch 179/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5132 - val_loss: 2391.1667\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 2390.50708\n",
      "Epoch 180/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4766 - val_loss: 2391.3550\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 2390.50708\n",
      "Epoch 181/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.7156 - val_loss: 2390.5059\n",
      "\n",
      "Epoch 00181: val_loss improved from 2390.50708 to 2390.50586, saving model to best_model.h5\n",
      "Epoch 182/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5388 - val_loss: 2390.8716\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 2390.50586\n",
      "Epoch 183/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4705 - val_loss: 2391.1013\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 2390.50586\n",
      "Epoch 184/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4282 - val_loss: 2390.6948\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 2390.50586\n",
      "Epoch 185/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4683 - val_loss: 2391.2454\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 2390.50586\n",
      "Epoch 186/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5874 - val_loss: 2391.3071\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 2390.50586\n",
      "Epoch 187/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3494 - val_loss: 2391.3555\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 2390.50586\n",
      "Epoch 188/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3887 - val_loss: 2390.7947\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 2390.50586\n",
      "Epoch 189/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4272 - val_loss: 2390.9512\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 2390.50586\n",
      "Epoch 190/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3564 - val_loss: 2390.9915\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 2390.50586\n",
      "Epoch 191/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3657 - val_loss: 2390.8337\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 2390.50586\n",
      "Epoch 192/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3586 - val_loss: 2391.2605\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 2390.50586\n",
      "Epoch 193/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3596 - val_loss: 2391.0894\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 2390.50586\n",
      "Epoch 194/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5298 - val_loss: 2391.1318\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 2390.50586\n",
      "Epoch 195/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4358 - val_loss: 2390.9937\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 2390.50586\n",
      "Epoch 196/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4373 - val_loss: 2390.6877\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 2390.50586\n",
      "Epoch 197/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3933 - val_loss: 2390.9543\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 2390.50586\n",
      "Epoch 198/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4329 - val_loss: 2391.1106\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 2390.50586\n",
      "Epoch 199/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5491 - val_loss: 2391.4624\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 2390.50586\n",
      "Epoch 200/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4546 - val_loss: 2391.3569\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 2390.50586\n",
      "Epoch 201/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4375 - val_loss: 2391.0696\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 2390.50586\n",
      "Epoch 202/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3896 - val_loss: 2390.9756\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 2390.50586\n",
      "Epoch 203/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4233 - val_loss: 2391.2673\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 2390.50586\n",
      "Epoch 204/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4194 - val_loss: 2391.0007\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 2390.50586\n",
      "Epoch 205/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3032 - val_loss: 2390.8645\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 2390.50586\n",
      "Epoch 206/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3213 - val_loss: 2390.9299\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 2390.50586\n",
      "Epoch 207/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3193 - val_loss: 2390.9785\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 2390.50586\n",
      "Epoch 208/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4548 - val_loss: 2390.9766\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 2390.50586\n",
      "Epoch 209/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2583 - val_loss: 2391.1260\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 2390.50586\n",
      "Epoch 210/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2939 - val_loss: 2391.0146\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 2390.50586\n",
      "Epoch 211/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4163 - val_loss: 2390.9963\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 2390.50586\n",
      "Epoch 212/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2720 - val_loss: 2390.9463\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 2390.50586\n",
      "Epoch 213/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2881 - val_loss: 2391.1282\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 2390.50586\n",
      "Epoch 214/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3042 - val_loss: 2390.8711\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 2390.50586\n",
      "Epoch 215/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5186 - val_loss: 2391.3274\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 2390.50586\n",
      "Epoch 216/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.1443 - val_loss: 2390.8538\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 2390.50586\n",
      "Epoch 217/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4099 - val_loss: 2390.9819\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 2390.50586\n",
      "Epoch 218/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3503 - val_loss: 2391.2043\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 2390.50586\n",
      "Epoch 219/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5139 - val_loss: 2391.3047\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 2390.50586\n",
      "Epoch 220/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.6462 - val_loss: 2390.7637\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 2390.50586\n",
      "Epoch 221/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3140 - val_loss: 2391.0925\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 2390.50586\n",
      "Epoch 222/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.6252 - val_loss: 2391.4009\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 2390.50586\n",
      "Epoch 223/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5906 - val_loss: 2391.2051\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 2390.50586\n",
      "Epoch 224/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4150 - val_loss: 2391.4158\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 2390.50586\n",
      "Epoch 225/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3159 - val_loss: 2391.0134\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 2390.50586\n",
      "Epoch 226/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2878 - val_loss: 2390.8936\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 2390.50586\n",
      "Epoch 227/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3411 - val_loss: 2391.2292\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 2390.50586\n",
      "Epoch 228/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5105 - val_loss: 2390.8477\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 2390.50586\n",
      "Epoch 229/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.6138 - val_loss: 2391.2349\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 2390.50586\n",
      "Epoch 230/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3079 - val_loss: 2390.7090\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 2390.50586\n",
      "Epoch 231/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3411 - val_loss: 2390.9836\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 2390.50586\n",
      "Epoch 232/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5325 - val_loss: 2390.9832\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 2390.50586\n",
      "Epoch 233/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2441 - val_loss: 2391.0481\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 2390.50586\n",
      "Epoch 234/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3860 - val_loss: 2391.1021\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 2390.50586\n",
      "Epoch 235/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5085 - val_loss: 2390.8418\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 2390.50586\n",
      "Epoch 236/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3159 - val_loss: 2391.2361\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 2390.50586\n",
      "Epoch 237/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4229 - val_loss: 2391.3501\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 2390.50586\n",
      "Epoch 238/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4490 - val_loss: 2391.3760\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 2390.50586\n",
      "Epoch 239/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2485 - val_loss: 2391.2607\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 2390.50586\n",
      "Epoch 240/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2212 - val_loss: 2391.0627\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 2390.50586\n",
      "Epoch 241/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5728 - val_loss: 2391.7327\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 2390.50586\n",
      "Epoch 242/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3174 - val_loss: 2390.9788\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 2390.50586\n",
      "Epoch 243/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2004 - val_loss: 2390.7710\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 2390.50586\n",
      "Epoch 244/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4744 - val_loss: 2391.0068\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 2390.50586\n",
      "Epoch 245/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3865 - val_loss: 2390.9773\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 2390.50586\n",
      "Epoch 246/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4368 - val_loss: 2391.2314\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 2390.50586\n",
      "Epoch 247/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.4971 - val_loss: 2391.0571\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 2390.50586\n",
      "Epoch 248/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5725 - val_loss: 2391.4424\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 2390.50586\n",
      "Epoch 249/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5042 - val_loss: 2391.2585\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 2390.50586\n",
      "Epoch 250/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2676 - val_loss: 2391.1846\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 2390.50586\n",
      "Epoch 251/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.5515 - val_loss: 2391.0342\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 2390.50586\n",
      "Epoch 252/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3420 - val_loss: 2391.1975\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 2390.50586\n",
      "Epoch 253/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.2378 - val_loss: 2391.2146\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 2390.50586\n",
      "Epoch 254/2000\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 2402.3457 - val_loss: 2390.9553\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 2390.50586\n",
      "Epoch 255/2000\n",
      " 51/100 [==============>...............] - ETA: 0s - loss: 2486.4802"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-af7228a3096b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtuner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexecutions_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random_search'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ANN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_space_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras_tuner/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras_tuner/engine/multi_execution_tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"min\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras_tuner/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, fit_args, fit_kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \"\"\"\n\u001b[1;32m    146\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(build_model,objective='val_loss',max_trials=100,executions_per_trial=1,directory='random_search', project_name='ANN')\n",
    "tuner.search(X_train, y_train, validation_data=(X_test, y_test),callbacks=[early_stopping,model_checkpoint],epochs=2000,batch_size = 10)\n",
    "\n",
    "tuner.search_space_summary()\n",
    "                  \n",
    "tuner.results_summary()\n",
    "models = tuner.get_best_models(num_models=1)\n",
    "best_model = models[0]\n",
    "loss = best_model.evaluate(X_test, y_test)  \n",
    "\n",
    "history = best_model.fit(X_train, y_train, epochs=2000,validation_data=(X_test,y_test), verbose=0,callbacks=[early_stopping,model_checkpoint], batch_size = 10)\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "model.summary()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
