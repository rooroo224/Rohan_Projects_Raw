{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy import signal\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Read the csv file\n",
    "\n",
    "The file is read as a numpy array. The first row which contains the label is deleted. If the data has five columns, then the first column is deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that takes the path of the single file and deletes the header and extra index row if present\n",
    "def read_file(file_path):\n",
    "    data= np.genfromtxt(file_path,delimiter=',')\n",
    "    data = np.delete(data,0,0)\n",
    "    if data.shape[1]==5:\n",
    "        data = np.delete(data,0,1)  #First column\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The gyroscope starts measuring the data after a few time later than the accelerometer.\n",
    "#So we have to delete the data in accelerometer which doesnot have corresponding gyroscope data.\n",
    "def sync_gyro_data(df1,df2):\n",
    "    diff = df1.shape[0] - df2.shape[0]\n",
    "    if diff == 0:\n",
    "        return df1,df2\n",
    "    elif diff == 1:\n",
    "        return df1[1:,:],df2\n",
    "    elif diff < 0:\n",
    "        return df1,df2[-diff:,:]                 #If gyroscope has more datapoint than accelerometer\n",
    "    else:\n",
    "        return df1[diff:,:],df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 : Calculate the frequency\n",
    "\n",
    "The column containing the time of the accelerometer is passed to the function to get the sampling frequency of the sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(t):\n",
    "    return t.shape[0]/t[t.shape[0]-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Filtering\n",
    "\n",
    "Butterworth filter is used to filter the very high frequency noise present in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df,freq):\n",
    "    sig = df[:,1:4]\n",
    "    sos = signal.butter(5, 50, 'lp', fs=2000, output='sos')\n",
    "    filtered = signal.sosfilt(sos, sig)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Cropping\n",
    "\n",
    "To find the point where the person starts walking, we find the gradient of any one component of accelerometer. Very small or zero gradient for a long time indicates that the person is not in motion. The time duration for which gradient must be less than a threshold value, thresh, is given by lenthresh. If the len becomes greater than lenthresh it indicates the that the person is still. For certain subjects who have not stood still for sometime before walking, there was no region of length greater than lenthresh where gradient was less than thresh. For such cases, the start point was found by finding a region which have value either negative or positive for a particular number of datapoints.\n",
    "The stop point were found in a similar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cropping\n",
    "def find_start_pt(x,thresh,lenthresh):\n",
    "    \n",
    "    gradx = np.gradient(x)\n",
    "    start_pt = 0\n",
    "    length = 0\n",
    "    for i in range(0,int(x.shape[0]/4)):\n",
    "        if np.abs(gradx[i]) < thresh:          #Check if gradient is less than a threshold value\n",
    "            length +=1\n",
    "            if length > lenthresh:             #Check the if length of time for which gradient<thresh is greater than lenthresh\n",
    "                start_pt = i\n",
    "        else:\n",
    "            length = 0                         # restart of length counter if gradient goes above threshold\n",
    "            \n",
    "    if start_pt<200:                            #If the start is at begining of the data, we make sure that it is not before the starting noise\n",
    "        for i in range(0,int(x.shape[0]/4)):\n",
    "            if x[i] < 0:\n",
    "                length +=1\n",
    "                if length > lenthresh:\n",
    "                    start_pt = i\n",
    "            else:\n",
    "                length = 0\n",
    "    return start_pt\n",
    "    \n",
    "            \n",
    "def find_stop_pt(x,thresh,lenthresh):\n",
    "    stop_pt = x.shape[0]\n",
    "    gradx = np.gradient(x)\n",
    "    temp_pt = 0\n",
    "    length = 0\n",
    "    ltext = 200\n",
    "    \n",
    "    \n",
    "    for i in range(int(x.shape[0]*3/4),x.shape[0]):\n",
    "        if x[i] > 0:\n",
    "            if length ==0:\n",
    "                temp_pt = i                 #Marks the starting point where value is positive\n",
    "            elif length > ltext:\n",
    "                stop_pt = temp_pt           #If lenthresh hold is reached, then the starting point(temp_pt) is taken as top point\n",
    "                break\n",
    "            length +=1\n",
    "        else:\n",
    "            length = 0\n",
    "            \n",
    "    length = 0\n",
    "    \n",
    "    for i in range(int(x.shape[0]*3/4),x.shape[0]):\n",
    "        if x[i] < 0:\n",
    "            if length ==0:\n",
    "                temp_pt = i\n",
    "            elif length > ltext:\n",
    "                if stop_pt > temp_pt:\n",
    "                    stop_pt = temp_pt\n",
    "                break\n",
    "            length +=1\n",
    "        else:\n",
    "            length = 0\n",
    "            \n",
    "    length = 0\n",
    "            \n",
    "    for i in range(int(x.shape[0]*3/4),x.shape[0]):\n",
    "        if np.abs(gradx[i]) < thresh:                  #Marks the starting point where gradient goes to less than threshold\n",
    "            if length ==0:\n",
    "                temp_pt = i\n",
    "            elif length > lenthresh:\n",
    "                if stop_pt > temp_pt:\n",
    "                    stop_pt = temp_pt\n",
    "                break\n",
    "            length +=1\n",
    "        else:\n",
    "            length = 0\n",
    "            \n",
    "    return stop_pt\n",
    "    \n",
    "def crop_data(df,df2):\n",
    "    \n",
    "    x = np.copy(df[:,1])\n",
    "    thresh = 0.05\n",
    "    lenthresh = 50\n",
    "    start_pt = find_start_pt(x,thresh,lenthresh)\n",
    "    stop_pt = find_stop_pt(x,thresh,lenthresh)\n",
    "#     print('Start pt : {}'.format(start_pt))\n",
    "#     print('stop pt : {}'.format(stop_pt))\n",
    "    return df[start_pt:stop_pt],df2[start_pt:stop_pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step:5 Rotation\n",
    "\n",
    "The data is scaled and centered so as to have zero mean and variance equal to one. And then the data set is rotated such that the acceleration is maximum along one axis. This is done to correct the different initial orientation of different smartphone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rotation\n",
    "\n",
    "def rotation(df):\n",
    "    df = preprocessing.scale(df)\n",
    "    \n",
    "    pca = PCA()\n",
    "    df = pca.fit_transform(df)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step6: Segmentation\n",
    "\n",
    "The peaks in the data are identified so that the whole motion sequence is segmented into different steps. To avoid selecting the smaller peaks, the distance between the peaks and the prominance is defined. To accomodate the fact that the frequency of the sensor in the smartphone differ, different distance criteria is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_peaks(a):\n",
    "    peaks = signal.find_peaks(a, distance = 200, height = 0.02578, prominence = 0.035)\n",
    "    if len(peaks[0]) >= 10:\n",
    "        return peaks\n",
    "    else:\n",
    "        peaks = signal.find_peaks(a, distance =30, height = 0.02578)\n",
    "        if len(peaks[0]) < 20:\n",
    "            peaks = signal.find_peaks(-a, distance = 100, height = 0.005, prominence = 0.035)\n",
    "        return peaks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step7: Resampling\n",
    "\n",
    "The peak data is take as input and the motion sequence is divided into steps or sample. Different samples have different number of data points but we only can give a fixed number of input to the nueral network, we resample such that all the sample have number of datapoints equal to 'data_points'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_data(rotated_data, peaks, data_points):\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    \n",
    "    datax = np.copy(rotated_data[:,0])\n",
    "    datay = np.copy(rotated_data[:,1])\n",
    "    dataz = np.copy(rotated_data[:,2])\n",
    "    \n",
    "    \n",
    "    #Dividing the motion sequence into steps\n",
    "    for i in range(len(peaks)-1):\n",
    "        x.append(np.copy(datax[peaks[i] : peaks[i+1]]))\n",
    "        y.append(np.copy(datay[peaks[i] : peaks[i+1]]))\n",
    "        z.append(np.copy(dataz[peaks[i] : peaks[i+1]]))\n",
    "        \n",
    "    interpx = []\n",
    "    interpy = []\n",
    "    interpz = []\n",
    "    \n",
    "    #Resampling\n",
    "    for i in range(len(x)):\n",
    "        interpx.append(np.copy(signal.resample(x[i], data_points)))\n",
    "        interpy.append(np.copy(signal.resample(y[i], data_points)))\n",
    "        interpz.append(np.copy(signal.resample(z[i], data_points)))\n",
    "    \n",
    "    return interpx,interpy,interpz\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Remove outliers\n",
    "\n",
    "The mean and standard deviation at each point is calculated. If the points in the sample lie outside (mean + or - std) then it is declared as an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(interpx, interpy, interpz,interpx1,interpy1,interpz1, peaks, data_points):\n",
    "        \n",
    "    sampledx = np.asarray(interpx)\n",
    "    avgx = np.average(interpx, axis=0)       #Average at each data point\n",
    "    avgx = avgx[:,np.newaxis]\n",
    "    avgx = avgx.T       \n",
    "    \n",
    "    diffx =  np.zeros((len(peaks)-1 , data_points))\n",
    "    \n",
    "    normx = np.zeros((len(peaks)-1, 1))\n",
    "    avg_normx = norm(avgx)\n",
    "    \n",
    "    for i in range(len(peaks)-1):\n",
    "        diffx[i,:] =  sampledx[i,:] - avgx[0,:]      #Differece between the sample and the average at each point\n",
    "        normx[i,0] = norm(diffx[i,:])\n",
    "    \n",
    "    remove_x = []\n",
    "    #If the differece is more than half of standard deviation then it is added to remove list \n",
    "    for i in range(len(peaks)-1):\n",
    "        if (((normx[i,0] -norm(avg_normx) ) <= 0.5*np.std(normx)) or ((normx[i,0] +norm(avg_normx) ) <= 0.5*np.std(normx))):\n",
    "            continue\n",
    "        else:\n",
    "            remove_x.append(i)\n",
    "            \n",
    "    #Sample in the remove list is deleted\n",
    "    interpx = np.delete(interpx,remove_x,axis=0)\n",
    "    interpy = np.delete(interpy,remove_x,axis=0)\n",
    "    interpz = np.delete(interpz,remove_x,axis=0)\n",
    "    interpx1 = np.delete(interpx1,remove_x,axis=0)\n",
    "    interpy1 = np.delete(interpy1,remove_x,axis=0)\n",
    "    interpz1 = np.delete(interpz1,remove_x,axis=0)\n",
    "       \n",
    "    \n",
    "    return interpx,interpy,interpz,interpx1,interpy1,interpz1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessor function takes the data of the accelerometer and gyroscope and applies the eight steps of preprocessing and returns a 2D array whose columns represent each step and row corresponds to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(df1,df2):\n",
    "    \n",
    "    df1,df2 = sync_gyro_data(df1,df2)\n",
    "    freq = frequency(df1[:,0:1])\n",
    "    \n",
    "    df1 = filter_data(df1,freq)\n",
    "    df2 = filter_data(df2,freq)\n",
    "    \n",
    "    df1,df2 = crop_data(df1,df2)\n",
    "    \n",
    "    df1 = rotation(df1)\n",
    "    df2 = rotation(df2)\n",
    "    \n",
    "    peaks = segmentation_peaks(df1[:,2])\n",
    "    peaks = peaks[0]\n",
    "    if peaks.shape[0] < 3:                     #If only one step is discovered then ignore the whole trial\n",
    "        return 0\n",
    "    \n",
    "    ndata_pts = 100\n",
    "    ix1,iy1,iz1 = resample_data(df1,peaks,ndata_pts)\n",
    "    ix2,iy2,iz2 = resample_data(df2,peaks,ndata_pts)\n",
    "    \n",
    "    \n",
    "    ix1,iy1,iz1,ix2,iy2,iz2 = remove_outliers(ix1,iy1,iz1,ix2,iy2,iz2,peaks,ndata_pts)\n",
    "    \n",
    "    out = np.zeros((len(ix1),ndata_pts,6))\n",
    "    for i in range(len(ix1)):\n",
    "        out[i,:,0] = ix1[i]\n",
    "        out[i,:,1] = iy1[i]\n",
    "        out[i,:,2] = iz1[i]\n",
    "        out[i,:,3] = ix2[i]\n",
    "        out[i,:,4] = iy2[i]\n",
    "        out[i,:,4] = iz2[i]\n",
    "    if out.shape[0] ==0:\n",
    "        out =0\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plotter funcion takes the file name of accelerometer and gyroscope as input and plots all the data. Some plots are done only for one component od acceleration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(file1,file2):\n",
    "    \n",
    "    df1 = read_file(file1)\n",
    "    df2 = read_file(file2)\n",
    "    df1,df2 = sync_gyro_data(df1,df2)\n",
    "    freq = frequency(df1[:,0:1])\n",
    "    \n",
    "    ind = 2\n",
    "    plt.figure()\n",
    "    plt.plot(df1[:,1])\n",
    "    plt.title(\"Raw data - AccX\")\n",
    "    plt.xlabel(\"Datapoints\")\n",
    "    plt.ylabel(\"Acceleration in x-axis\")\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(df1[:,2])\n",
    "    plt.title(\"Raw data - AccY\")\n",
    "    plt.xlabel(\"Datapoints\")\n",
    "    plt.ylabel(\"Acceleration in y-axis\")\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(df1[:,3])\n",
    "    plt.title(\"Raw data - AccZ\")\n",
    "    plt.xlabel(\"Datapoints\")\n",
    "    plt.ylabel(\"Acceleration in z-axis\")\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(df2[:,1])\n",
    "    plt.title(\"Raw data - GyrX\")\n",
    "    plt.xlabel(\"Datapoints\")\n",
    "    plt.ylabel(\"Gyroscope value in x-axis\")\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(df2[:,2])\n",
    "    plt.title(\"Raw data - GyrY\")\n",
    "    plt.xlabel(\"Datapoints\")\n",
    "    plt.ylabel(\"Gyroscope value in y-axis\")\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(df2[:,3])\n",
    "    plt.title(\"Raw data - GyrZ\")\n",
    "    plt.xlabel(\"Datapoints\")\n",
    "    plt.ylabel(\"Gyroscope value in z-axis\")\n",
    "    \n",
    "    df1 = filter_data(df1,freq)\n",
    "    df2 = filter_data(df2,freq)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(df1[:,ind])\n",
    "    plt.title(\"Filtered data\")\n",
    "    plt.xlabel(\"Datapoints\")\n",
    "    plt.ylabel(\"Acceleration\")\n",
    "    \n",
    "    \n",
    "    gradx = np.gradient(df1[:,ind])\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(gradx)\n",
    "    plt.title(\"Gradient data\")\n",
    "    plt.xlabel(\"Datapoints\")\n",
    "    plt.ylabel(\"Acceleration\")\n",
    "    \n",
    "    df1,df2 = crop_data(df1,df2)  \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(df1[:,ind])\n",
    "    plt.title(\"Cropped data\")\n",
    "    plt.xlabel(\"Datapoints\")\n",
    "    plt.ylabel(\"Acceleration\")\n",
    "    \n",
    "    df1 = rotation(df1)\n",
    "    df2 = rotation(df2)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(df1[:,ind])\n",
    "    plt.title(\"Rotated data\")\n",
    "    plt.xlabel(\"Datapoints\")\n",
    "    plt.ylabel(\"Acceleration\")\n",
    "    \n",
    "    peaks = segmentation_peaks(df1[:,ind])\n",
    "    peaks = peaks[0]\n",
    "    print('Peaks: {}'.format(peaks.shape))\n",
    "    if peaks.shape[0] < 3:\n",
    "        return 0\n",
    "    y = np.zeros(len(peaks))\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(df1[:,ind])\n",
    "    plt.title(\"Segmentation\")\n",
    "    plt.xlabel(\"Datapoints\")\n",
    "    plt.ylabel(\"Acceleration\")\n",
    "\n",
    "    cntr =0\n",
    "    for i in peaks:   \n",
    "        y[cntr] = df1[i,ind]\n",
    "        cntr +=1\n",
    "    \n",
    "    plt.plot(peaks,y,'o')\n",
    "    \n",
    "    ndata_pts = 100\n",
    "    ix1,iy1,iz1 = resample_data(df1,peaks,ndata_pts)\n",
    "    ix2,iy2,iz2 = resample_data(df2,peaks,ndata_pts)\n",
    "    \n",
    "    plt.figure()\n",
    "    for i in range(len(ix1)):\n",
    "        plt.plot(ix1[i])\n",
    "    plt.title('Resampled data: Acceleration in x-axis')\n",
    "        \n",
    "    plt.figure()\n",
    "    for i in range(len(ix1)):\n",
    "        plt.plot(iy1[i])\n",
    "    plt.title('Resampled data: Acceleration in y-axis')\n",
    "        \n",
    "    plt.figure()\n",
    "    for i in range(len(ix1)):\n",
    "        plt.plot(iz1[i])\n",
    "    plt.title('Resampled data: Acceleration in z-axis')\n",
    "    \n",
    "\n",
    "    ix1,iy1,iz1,ix2,iy2,iz2 = remove_outliers(ix1,iy1,iz1,ix2,iy2,iz2,peaks,ndata_pts)\n",
    "    \n",
    "    plt.figure()\n",
    "    for i in range(len(ix1)):\n",
    "        plt.plot(ix1[i])\n",
    "    plt.title('Samples after removing outliers: Acceleration in x-axis')\n",
    "        \n",
    "    plt.figure()\n",
    "    for i in range(len(ix1)):\n",
    "        plt.plot(iy1[i])\n",
    "    plt.title('Samples after removing outliers: Acceleration in y-axis')\n",
    "        \n",
    "    plt.figure()\n",
    "    for i in range(len(ix1)):\n",
    "        plt.plot(iz1[i])\n",
    "    plt.title('Samples after removing outliers: Acceleration in z-axis')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Main\n",
    "\n",
    "We loop over all the file and preprocess each file and return the output as 'out'. Then the sex and or age is added as a last column and the output saved as 'out' and 'out_age'.\n",
    "\n",
    "Output from each file is added to the list 'inp' and 'inp_age'. and the subject numbers(Needed to seperate them into five subject wise sets) and the label is added to the list 'inp_sub' and 'y'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flag = 1\n",
    "subject = range(25,140)\n",
    "\n",
    "gait = ['normal','impaired']\n",
    "trials = ['01','02','03']\n",
    "inp = []\n",
    "inp_sub = []\n",
    "inp_age = []\n",
    "y = []\n",
    "file0= 'data/anthro.xlsx'\n",
    "\n",
    "#Read athropology file\n",
    "anthro_df = pd.read_excel(file0)\n",
    "anthro_arr = anthro_df.to_numpy()\n",
    "nan = anthro_arr[142,1]\n",
    "\n",
    "#extract subject sum into a array\n",
    "participant_num = anthro_arr[:,0]\n",
    "for i in range(len(participant_num)):\n",
    "    cut = participant_num[i] \n",
    "    participant_num[i] = cut[1:]\n",
    "\n",
    "sex = anthro_arr[:,1]\n",
    "age = anthro_arr[:,2]\n",
    "\n",
    "#read the sex and assign number accordingly\n",
    "for i in range(len(sex)):\n",
    "    if type(sex[i]) == type('str'):\n",
    "        sex[i] = sex[i].upper()\n",
    "        if (sex[i] == 'MALE' or sex[i] == 'M'):\n",
    "            sex[i] = 1  \n",
    "        else:\n",
    "            sex[i] = 0\n",
    "\n",
    "            \n",
    "for i in subject:\n",
    "    if type(sex[i]) != type(1):    #Do the iteration only if sex value is given\n",
    "        continue\n",
    "    \n",
    "    for j in trials:\n",
    "        for k in gait:\n",
    "            if flag:\n",
    "                try:\n",
    "                    file1 = os.getcwd()+'/data/Smartphone3/subject{}'.format(i)+'_'+k+j+'/Accelerometer.csv'\n",
    "                    df1 = read_file(file1)\n",
    "                    file2 = os.getcwd()+'/data/Smartphone3/subject{}'.format(i)+'_'+k+j+'/Gyroscope.csv'\n",
    "                    df2 = read_file(file2)\n",
    "\n",
    "                except OSError:\n",
    "                    flag = 0  #Prevent from throwing an error if file is not present and the subject will not be taken into consideration\n",
    "                if flag:\n",
    "                    out = preprocessor(df1,df2)\n",
    "                    if type(out) != int:     #To skip cases where there is not output\n",
    "                        print('Data: subject:{},gait:{},trials:{} read'.format(i,k,j))\n",
    "                        if sex[i]:\n",
    "                            s = np.ones((out.shape[0],100,1))\n",
    "                        else:\n",
    "                            s = np.zeros((out.shape[0],100,1))\n",
    "                        age_vec = age[i] * np.ones((out.shape[0],100,1))\n",
    "                        out_age = np.concatenate((out,age_vec),axis=2)\n",
    "                        out = np.concatenate((out,s),axis=2)\n",
    "                        if k ==\"normal\":\n",
    "                            y.append(np.zeros((out.shape[0],1)))\n",
    "                        elif k == 'impaired':\n",
    "                            y.append(np.ones((out.shape[0],1)))\n",
    "                        inp.append(np.copy(out))\n",
    "                        inp_age.append(np.copy(out_age))\n",
    "                        inp_sub.append(np.copy(i))\n",
    "                        \n",
    "    flag = 1     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Give the index where the set starts\n",
    "def give_start_index(sub,inp_sub):\n",
    "    cntr=0\n",
    "    for i in inp_sub:\n",
    "        if i==sub or i>sub:\n",
    "            return cntr\n",
    "        cntr += 1\n",
    "    return cntr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For cross validation the data have to be divided to five sets according to the subjects\n",
    "sub_per_set = int(len(subject)/5)       # No. of subjects per set\n",
    "\n",
    "#Give the starting subject in each subject\n",
    "sub_start = [i for i in subject if ((i-25)%sub_per_set==0)]\n",
    "\n",
    "#give index of the starting element of each set in inp list\n",
    "id_start = [give_start_index(i,inp_sub) for i in sub_start]\n",
    "id_start.append(len(inp_sub))\n",
    "subset = []\n",
    "subset_age = []\n",
    "label = []\n",
    "\n",
    "#concatenate all the sample in one set to an array and store it as one element of the list\n",
    "for i in range(5):\n",
    "    subset.append(np.copy(inp[id_start[i]]))\n",
    "    subset_age.append(np.copy(inp_age[id_start[i]]))\n",
    "    label.append(np.copy(y[id_start[i]]))\n",
    "    for j in range(id_start[i]+1,id_start[i+1]):\n",
    "        subset[i] = np.concatenate((subset[i],inp[j]),axis=0)\n",
    "        subset_age[i] = np.concatenate((subset_age[i],inp_age[j]),axis=0)\n",
    "        label[i] = np.concatenate((label[i],y[j]),axis=0)\n",
    "    \n",
    "print(subset[1].shape)\n",
    "print(label[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select a subject to plot the graphs\n",
    "i = 34\n",
    "k = \"normal\"\n",
    "j = \"01\"\n",
    "file1 = os.getcwd()+'/data/Smartphone3/subject{}'.format(i)+'_'+k+j+'/Accelerometer.csv'\n",
    "file2 = os.getcwd()+'/data/Smartphone3/subject{}'.format(i)+'_'+k+j+'/Gyroscope.csv'\n",
    "\n",
    "plotter(file1,file2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before passing the data to the neural network, it must be normalized. We have normalized such that the value lies between 0.1 and 0.9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(df,age_flag=False):\n",
    "    df[:,:,5] = 0.1* np.ones_like(df[:,:,5])   #Since all the element of gyroscope z is zero\n",
    "    for i in range(5):\n",
    "        a = np.copy(df[:,:,i])\n",
    "        df[:,:,i] = 0.1 * np.ones_like(a) + 0.8 * (a - np.min(a)* np.ones_like(a))/(np.max(a)-np.min(a))\n",
    "    # Sex value is either 0 or 1, so there is no need of normalization. But for age the normalization is required\n",
    "    if(age_flag == True):     \n",
    "        a = np.copy(df[:,:,6])\n",
    "        df[:,:,6] = 0.1 * np.ones_like(a) + 0.8 * (a - np.min(a)* np.ones_like(a))/(np.max(a)-np.min(a))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network based on sex \n",
    "# Function to create the NN model, train it and the evaluate it. Input parameters are training dataset and test dataset\n",
    "#Number of input neuron = 700, hidden layer= 3, neuron per hidden layer = [7,6,5], output neuron = 1\n",
    "\n",
    "\n",
    "def NN_sex(trains,trainl,tests,testl,epoch):\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(100,7)),            #Input layer\n",
    "    tf.keras.layers.Dense(7, activation ='sigmoid'),         #Hidden layer 1\n",
    "    tf.keras.layers.Dense(6, activation ='sigmoid'),         #Hidden layer 2\n",
    "    tf.keras.layers.Dense(5, activation ='sigmoid'),         #Hidden layer 3\n",
    "    tf.keras.layers.Dense(1, activation ='sigmoid')          #Output layer\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer = 'adam',\n",
    "                 loss = 'binary_crossentropy',\n",
    "                 metrics = ['accuracy'])\n",
    "\n",
    "    model.fit(trains, trainl, epochs = epoch, shuffle=True, batch_size=5);\n",
    "    \n",
    "    loss, accuracy = model.evaluate(tests, testl, verbose=0);\n",
    "\n",
    "    print('Loss=', loss)\n",
    "    print('accuracy=', accuracy)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network based on age\n",
    "# Function to create the NN model, train it and the evaluate it. Input parameters are training dataset and test dataset\n",
    "#Number of input neuron = 700, hidden layer= 3, neuron per hidden layer = [7,6,5], output neuron = 1\n",
    "\n",
    "def NN_age(trains,trainl,tests,testl,epoch):\n",
    "    \n",
    "    model1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(100,7)),\n",
    "    tf.keras.layers.Dense(7, activation ='tanh'),\n",
    "    tf.keras.layers.Dense(6, activation ='tanh'),\n",
    "    tf.keras.layers.Dense(5, activation ='sigmoid'),\n",
    "    tf.keras.layers.Dense(1, activation ='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model1.compile(optimizer = 'adam',\n",
    "                 loss = 'binary_crossentropy',\n",
    "                 metrics = ['accuracy'])\n",
    "\n",
    "    model1.fit(trains, trainl, epochs = epoch, shuffle=True, batch_size=5);\n",
    "    \n",
    "    loss, accuracy = model1.evaluate(tests, testl, verbose=0);\n",
    "\n",
    "    print('Loss=', loss)\n",
    "    print('accuracy=', accuracy)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Cross validation\n",
    "\n",
    "Five fold subject wise cross validation is performed. The list subset contains the input data in five sets. We loop over the index of dataset. We take that set as the test dataset and concatenate other input data into training dataset. Normalization is done and then the trainng and test dataset are passed to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation with sex\n",
    "epoch = 3\n",
    "avg_acc = 0.0\n",
    "max_acc = 0.0\n",
    "\n",
    "for i in range(0,5):\n",
    "    trainlist = [0,1,2,3,4]\n",
    "    trainlist.remove(i)\n",
    "    trainset = np.concatenate((subset[trainlist[0]],subset[trainlist[1]],subset[trainlist[2]],subset[trainlist[3]]),axis=0)\n",
    "    trainlabel = np.concatenate((label[trainlist[0]],label[trainlist[1]],label[trainlist[2]],label[trainlist[3]]),axis=0)\n",
    "    testset = np.copy(subset[i])\n",
    "    testlabel = np.copy(label[i])\n",
    "    trainset = normalization(trainset)\n",
    "    testset = normalization(testset)\n",
    "    NN_sex(trainset,trainlabel,testset,testlabel,epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Cross validation with age\n",
    "epoch = 3\n",
    "avg_acc = 0.0\n",
    "max_acc = 0.0\n",
    "for i in range(0,5):\n",
    "    trainlist = [0,1,2,3,4]\n",
    "    trainlist.remove(i)\n",
    "    trainset_age = np.concatenate((subset_age[trainlist[0]],subset_age[trainlist[1]],subset_age[trainlist[2]],subset_age[trainlist[3]]),axis=0)\n",
    "    trainlabel_age = np.concatenate((label[trainlist[0]],label[trainlist[1]],label[trainlist[2]],label[trainlist[3]]),axis=0)\n",
    "    testset_age = np.copy(subset_age[i])\n",
    "    testlabel_age = np.copy(label[i])\n",
    "    \n",
    "    trainset_age = normalization(np.copy(trainset_age),age_flag=True)\n",
    "    testset_age = normalization(np.copy(testset_age),age_flag=True)\n",
    "\n",
    "    NN_age(trainset_age,trainlabel_age,testset_age,testlabel_age,epoch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
